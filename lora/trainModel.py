# loraë¥¼ í™œìš©í•´ íŒŒì¸íŠœë‹
from datasets import load_dataset, load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import get_peft_model, LoraConfig, TaskType

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
model_path = "/Users/Shared/app/llm/model/origin/kogpt2-base-v2"
data_path = "/Users/Shared/app/llm/datasets/preprocess/AI_HUB_legal_QA_data"
finetune_model_path = "/Users/Shared/app/llm/model/finetune"

# 2. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token  # ğŸ’¡ í•µì‹¬ ì„¤ì •
model = AutoModelForCausalLM.from_pretrained(model_path)

# 3. ë°ì´í„°ì…‹ ë¡œë“œ ë° êµ¬ì¡° í™•ì¸
dataset = load_from_disk(data_path)
print("âœ… ë°ì´í„° ìƒ˜í”Œ êµ¬ì¡°:", dataset[0].keys())  # 'input_ids', 'attention_mask', 'labels' í™•ì¸

# 4. ëª¨ë¸ êµ¬ì¡° ì¶œë ¥ (LoRA íƒ€ê¹ƒ ëª¨ë“ˆ í™•ì¸ìš©)
print("âœ… ì²« ë²ˆì§¸ attention ëª¨ë“ˆ êµ¬ì¡°:")
print(model.transformer.h[0].attn)

# 5. LoRA êµ¬ì„±
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["c_attn"],  # ë˜ëŠ” ['c_attn'] â†’ êµ¬ì¡° ë³´ê³  ë§ì¶° ì¡°ì •  "q_proj", "v_proj"
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# 6. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir=finetune_model_path,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    save_steps=500,
    save_total_limit=2,
    fp16=False  # âš ï¸ MPS ì‚¬ìš©ì‹œ ë°˜ë“œì‹œ False
)

# 7. ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# 8. íŠ¸ë ˆì´ë„ˆ êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)


# í•™ìŠµ ì‹œì‘
trainer.train()

# ëª¨ë¸ ì €ì¥
model.save_pretrained(finetune_model_path)
tokenizer.save_pretrained(finetune_model_path)