from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "skt/kogpt2-base-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()

# ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
# prompt = "ì‹ ìš©ì¹´ë“œë¥¼ ë¶„ì‹¤í–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n\n### ë‹µë³€:"
prompt = "ì‹ ìš©ì¹´ë“œë¥¼ ë¶„ì‹¤í–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"

# í† í°í™”
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"]

# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
input_ids = input_ids.to(device)

# í…ìŠ¤íŠ¸ ìƒì„±
with torch.no_grad():
    output = model.generate(
        input_ids,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.8,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id,
    )

# ê²°ê³¼ ë””ì½”ë”©
result = tokenizer.decode(output[0], skip_special_tokens=True)
print("ğŸ” ì§ˆë¬¸:\n", prompt)
print("ğŸ§  ì‘ë‹µ:\n", result)

# text = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
# match = re.search(r"### ë‹µë³€:(.*?)(###|\Z)", text, re.DOTALL)
# if match:
#     answer = match.group(1).strip()
#     print("ğŸ” ì§ˆë¬¸:\n", prompt)
#     print("ğŸ§  ë‹µë³€:\n", answer)
# else:
#     print("ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”.")