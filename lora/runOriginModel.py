from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re


def runModel(modelPath, prompt):
    tokenizer = AutoTokenizer.from_pretrained(modelPath, device_map="auto", quantization_config=None)
    model = AutoModelForCausalLM.from_pretrained(modelPath, device_map="auto", quantization_config=None)
    model.eval()

    # í† í°í™”
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"]

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ
    # device = "cuda" if torch.cuda.is_available() else "cpu"
    device = "mps" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    input_ids = input_ids.to(device)

    # í…ìŠ¤íŠ¸ ìƒì„±
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            top_k=50,
            eos_token_id=tokenizer.eos_token_id,
        )

    # ê²°ê³¼ ë””ì½”ë”©
    result = tokenizer.decode(output[0], skip_special_tokens=True)
    print("ğŸ” ì§ˆë¬¸:\n", prompt)
    print("ğŸ§  ì‘ë‹µ:\n", result)

    # text = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
    # match = re.search(r"### ë‹µë³€:(.*?)(###|\Z)", text, re.DOTALL)
    # if match:
    #     answer = match.group(1).strip()
    #     print("ğŸ” ì§ˆë¬¸:\n", prompt)
    #     print("ğŸ§  ë‹µë³€:\n", answer)
    # else:
    #     print("ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”.")


from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
def modelTest(modelPath):
    tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
    model = AutoModelForCausalLM.from_pretrained(modelPath, torch_dtype=torch.float16)

    input = tokenizer("ë¹„ì”¨ì¹´ë“œëŠ” ì–´ë–¤ ì€í–‰ì´ ìˆì–´?", return_tensors="pt")
    output = model.generate(**input, max_length=100, do_sample=True, top_k=40, top_p=0.9, temperature=0.7)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

if __name__ == "__main__":
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
    # modelPath = "/Users/Shared/app/llm/huggingface/kogpt2-base-v2"
    modelPath = "/Users/Shared/app/llm/lora/model"

    # ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # prompt = "ì‹ ìš©ì¹´ë“œë¥¼ ë¶„ì‹¤í–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n\n### ë‹µë³€:"
    prompt = """
    ë„ˆëŠ” ë¹„ì”¨ì¹´ë“œ ê´€ë ¨ ëª¨ë“  ì •ë³´ë¥¼ ì•Œê³ ìˆì–´ì„œ ë¬¸ë§¥ì— ë§ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼.
    
    ì§ˆë¬¸: ë¹„ì”¨ì¹´ë“œì¤‘ ëŒ€ì¶œ ì´ìœ¨ì´ ê°€ì¥ ë†’ì€ ìƒí’ˆì€ ë­ì•¼?
     
    ì‘ë‹µ:
    """

    runModel(modelPath, prompt)
    # modelTest(modelPath)